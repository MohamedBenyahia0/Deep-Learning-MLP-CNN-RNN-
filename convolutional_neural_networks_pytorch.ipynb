{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaht-FPA1Jvq"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "## Lab2: Train a Convolutional Neural Network (CNN).\n",
        "\n",
        "In this Lab session we will learn how to train a CNN from scratch for classifying MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UvxtTYHlVfRK"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYCvhGxKWyN7"
      },
      "source": [
        "### Define LeNet\n",
        "\n",
        "![network architecture](https://www.researchgate.net/profile/Lucijano-Berus/publication/329891470/figure/fig1/AS:707347647307776@1545656229128/Architecture-of-LeNet-5-a-Convolutional-Neural-Network-for-digits-digits-recognition-An.ppm)\n",
        "\n",
        "Here we are going to define our first CNN which is **LeNet** in this case. This architecture has been introduced and is detailed in [this article](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). To construct a LeNet we will be using some convolutional layers followed by some fully-connected layers. The convolutional layers can be simply defined using `torch.nn.Conv2d` module of `torch.nn` package. Details can be found [here](https://pytorch.org/docs/stable/nn.html#conv2d). Moreover, we will use pooling operation to reduce the size of convolutional feature maps. For this case we are going to use `torch.nn.functional.max_pool2d`. Details about maxpooling can be found [here](https://pytorch.org/docs/stable/nn.html#max-pool2d)\n",
        "\n",
        "Differently from our previous Lab, we will use a Rectified Linear Units (ReLU) as activation function with the help of `torch.nn.functional.relu`, replacing `torch.nn.Sigmoid`. Details about ReLU can be found [here](https://pytorch.org/docs/stable/nn.html#id26)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "dMC_LDYdWkI7"
      },
      "outputs": [],
      "source": [
        "class LeNet(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(LeNet, self).__init__()\n",
        "\n",
        "    # input channel = ?, output channels = ?, kernel size = ?\n",
        "    # input image size = (?, ?), image output size = (?, ?)\n",
        "    # TODO\n",
        "    input_channels=1\n",
        "    output_channels=6\n",
        "    kernel_size=5\n",
        "    self.conv1=torch.nn.Conv2d(input_channels,output_channels,kernel_size,padding='same')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # input channel = ?, output channels = ?, kernel size = ?\n",
        "    # input image size = (?, ?), output image size = (?, ?)\n",
        "    # TODO\n",
        "    input_channels=6\n",
        "    output_channels=16\n",
        "    kernel_size=5\n",
        "    self.conv2=torch.nn.Conv2d(input_channels,output_channels,kernel_size,padding='valid')\n",
        "\n",
        "\n",
        "\n",
        "    # input dim = ? ( H x W x C), output dim = ?\n",
        "    # TODO\n",
        "    self.linear1=torch.nn.Linear(16*5*5,120)\n",
        "\n",
        "    # input dim = ?, output dim = ?\n",
        "    # TODO\n",
        "    self.linear2=torch.nn.Linear(120,84)\n",
        "\n",
        "    # input dim = ?, output dim = ?\n",
        "    # TODO\n",
        "    self.linear3=torch.nn.Linear(84,10)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # TODO\n",
        "    # Max Pooling with kernel size = ?\n",
        "    # output size = (?, ?)\n",
        "    # TODO\n",
        "    x=self.conv1(x)\n",
        "\n",
        "\n",
        "    x=F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # TODO\n",
        "    # Max Pooling with kernel size = ?\n",
        "    # output size = (?, ?)\n",
        "    # TODO\n",
        "    x=self.conv2(x)\n",
        "\n",
        "\n",
        "    x=F.max_pool2d(x,kernel_size=2,stride=2)\n",
        "\n",
        "\n",
        "    # flatten the feature maps into a long vector\n",
        "    x = x.view(x.shape[0], -1)\n",
        "\n",
        "\n",
        "    # TODO\n",
        "    x=self.linear1(x)\n",
        "    x=F.relu(x)\n",
        "\n",
        "    # TODO\n",
        "    x=self.linear2(x)\n",
        "    x=F.relu(x)\n",
        "\n",
        "    # TODO\n",
        "    x=self.linear3(x)\n",
        "    x=F.softmax(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gChf6TvWonrV"
      },
      "source": [
        "### Define cost function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6j5UrBH3oek8"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  cost_function =  torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2TjXeVdorV9"
      },
      "source": [
        "### Define the optimizer\n",
        "\n",
        "We will use SGD with learning rate-lr, weight_decay=wd and  momentum=momentum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "hBZN-WPboulR"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum,weight_decay=wd)\n",
        "  return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTkfrV64oxIL"
      },
      "source": [
        "### Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t-sE5vFio0lf"
      },
      "outputs": [],
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  net.eval() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "\n",
        "  net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # Reset the optimizer\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6IT0Lsgo8AM"
      },
      "source": [
        "### Define the function that fetches a data loader that is then used during iterative training.\n",
        "\n",
        "We will learn a new thing in this function as how to Normalize the inputs given to the network.\n",
        "\n",
        "***Why Normalization is needed***?\n",
        "\n",
        "To have nice and stable training of the network it is recommended to normalize the network inputs between \\[-1, 1\\].\n",
        "\n",
        "***How it can be done***?\n",
        "\n",
        "This can be simply done using `torchvision.transforms.Normalize()` transform. Details can be found [here](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Normalize)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qDxpo6uVo_8k"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "\n",
        "  # Prepare data transformations and then combine them sequentially\n",
        "  transform = list()\n",
        "  transform.append(T.ToTensor())                            # converts Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=[0.5], std=[0.5]))      # Normalizes the Tensors between [-1, 1]\n",
        "  transform = T.Compose(transform)                          # Composes the above transformations into one.\n",
        "\n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
        "  test_data = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
        "\n",
        "\n",
        "  # Create train and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.5+1)\n",
        "  validation_samples = num_samples - training_samples\n",
        "\n",
        "  training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHcB8f0AsY4n"
      },
      "source": [
        "### Wrapping everything up\n",
        "\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ip_R-hruse0Q"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input arguments\n",
        "  batch_size: Size of a mini-batch\n",
        "  device: GPU where you want to train your network\n",
        "  weight_decay: Weight decay co-efficient for regularization of weights\n",
        "  momentum: Momentum for SGD optimizer\n",
        "  epochs: Number of epochs for training the network\n",
        "'''\n",
        "\n",
        "def main(batch_size=128,\n",
        "         device='cuda:0',\n",
        "         learning_rate=0.01,\n",
        "         weight_decay=0.000001,\n",
        "         momentum=0.9,\n",
        "         epochs=50):\n",
        "\n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "\n",
        "  # TODO for defining LeNet-5\n",
        "  net=LeNet()\n",
        "  net=net.to(device)\n",
        "\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltdCMiB3t18h"
      },
      "source": [
        "Lets train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d-z20H4tziL",
        "outputId": "02703705-1b17-429c-8532-e859106235ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before training:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-65-d352dbc8c7b4>:76: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  x=F.softmax(x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t Training loss 0.01804, Training accuracy 6.41\n",
            "\t Validation loss 0.00906, Validation accuracy 6.66\n",
            "\t Test loss 0.00921, Test accuracy 6.28\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.01801, Training accuracy 16.17\n",
            "\t Validation loss 0.00902, Validation accuracy 22.86\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.01598, Training accuracy 44.14\n",
            "\t Validation loss 0.00660, Validation accuracy 79.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.01287, Training accuracy 82.40\n",
            "\t Validation loss 0.00641, Validation accuracy 83.43\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.01265, Training accuracy 84.94\n",
            "\t Validation loss 0.00635, Validation accuracy 84.96\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.01255, Training accuracy 86.14\n",
            "\t Validation loss 0.00629, Validation accuracy 86.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.01248, Training accuracy 87.05\n",
            "\t Validation loss 0.00627, Validation accuracy 87.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.01242, Training accuracy 87.74\n",
            "\t Validation loss 0.00625, Validation accuracy 87.37\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.01239, Training accuracy 88.11\n",
            "\t Validation loss 0.00626, Validation accuracy 87.17\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.01236, Training accuracy 88.46\n",
            "\t Validation loss 0.00623, Validation accuracy 87.81\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.01234, Training accuracy 88.67\n",
            "\t Validation loss 0.00621, Validation accuracy 88.26\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.01232, Training accuracy 88.88\n",
            "\t Validation loss 0.00621, Validation accuracy 88.29\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.01230, Training accuracy 89.20\n",
            "\t Validation loss 0.00621, Validation accuracy 88.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.01229, Training accuracy 89.31\n",
            "\t Validation loss 0.00620, Validation accuracy 88.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.01227, Training accuracy 89.48\n",
            "\t Validation loss 0.00620, Validation accuracy 88.59\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.01226, Training accuracy 89.58\n",
            "\t Validation loss 0.00619, Validation accuracy 88.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.01225, Training accuracy 89.73\n",
            "\t Validation loss 0.00619, Validation accuracy 88.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.01224, Training accuracy 89.82\n",
            "\t Validation loss 0.00620, Validation accuracy 88.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.01224, Training accuracy 89.89\n",
            "\t Validation loss 0.00618, Validation accuracy 89.01\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.01223, Training accuracy 89.99\n",
            "\t Validation loss 0.00618, Validation accuracy 88.90\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.01222, Training accuracy 90.06\n",
            "\t Validation loss 0.00619, Validation accuracy 88.72\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.01222, Training accuracy 90.08\n",
            "\t Validation loss 0.00616, Validation accuracy 89.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.01166, Training accuracy 97.58\n",
            "\t Validation loss 0.00586, Validation accuracy 97.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.01159, Training accuracy 98.32\n",
            "\t Validation loss 0.00586, Validation accuracy 97.32\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.01157, Training accuracy 98.56\n",
            "\t Validation loss 0.00584, Validation accuracy 97.71\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.01156, Training accuracy 98.66\n",
            "\t Validation loss 0.00585, Validation accuracy 97.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.01155, Training accuracy 98.74\n",
            "\t Validation loss 0.00584, Validation accuracy 97.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.01154, Training accuracy 98.94\n",
            "\t Validation loss 0.00584, Validation accuracy 97.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.01154, Training accuracy 98.92\n",
            "\t Validation loss 0.00584, Validation accuracy 97.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.01154, Training accuracy 98.96\n",
            "\t Validation loss 0.00584, Validation accuracy 97.82\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.01153, Training accuracy 99.03\n",
            "\t Validation loss 0.00584, Validation accuracy 97.76\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.01152, Training accuracy 99.12\n",
            "\t Validation loss 0.00583, Validation accuracy 97.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.01152, Training accuracy 99.19\n",
            "\t Validation loss 0.00583, Validation accuracy 98.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.01151, Training accuracy 99.27\n",
            "\t Validation loss 0.00583, Validation accuracy 98.11\n",
            "-----------------------------------------------------\n",
            "Epoch: 34\n",
            "\t Training loss 0.01150, Training accuracy 99.33\n",
            "\t Validation loss 0.00582, Validation accuracy 98.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 35\n",
            "\t Training loss 0.01151, Training accuracy 99.30\n",
            "\t Validation loss 0.00582, Validation accuracy 98.10\n",
            "-----------------------------------------------------\n",
            "Epoch: 36\n",
            "\t Training loss 0.01151, Training accuracy 99.29\n",
            "\t Validation loss 0.00583, Validation accuracy 98.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 37\n",
            "\t Training loss 0.01150, Training accuracy 99.36\n",
            "\t Validation loss 0.00582, Validation accuracy 98.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 38\n",
            "\t Training loss 0.01150, Training accuracy 99.38\n",
            "\t Validation loss 0.00582, Validation accuracy 98.19\n",
            "-----------------------------------------------------\n",
            "Epoch: 39\n",
            "\t Training loss 0.01149, Training accuracy 99.43\n",
            "\t Validation loss 0.00583, Validation accuracy 97.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 40\n",
            "\t Training loss 0.01149, Training accuracy 99.43\n",
            "\t Validation loss 0.00583, Validation accuracy 98.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 41\n",
            "\t Training loss 0.01149, Training accuracy 99.47\n",
            "\t Validation loss 0.00583, Validation accuracy 98.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 42\n",
            "\t Training loss 0.01149, Training accuracy 99.47\n",
            "\t Validation loss 0.00582, Validation accuracy 98.21\n",
            "-----------------------------------------------------\n",
            "Epoch: 43\n",
            "\t Training loss 0.01149, Training accuracy 99.50\n",
            "\t Validation loss 0.00582, Validation accuracy 98.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 44\n",
            "\t Training loss 0.01149, Training accuracy 99.53\n",
            "\t Validation loss 0.00582, Validation accuracy 98.09\n",
            "-----------------------------------------------------\n",
            "Epoch: 45\n",
            "\t Training loss 0.01149, Training accuracy 99.50\n",
            "\t Validation loss 0.00582, Validation accuracy 98.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 46\n",
            "\t Training loss 0.01149, Training accuracy 99.54\n",
            "\t Validation loss 0.00582, Validation accuracy 98.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 47\n",
            "\t Training loss 0.01148, Training accuracy 99.55\n",
            "\t Validation loss 0.00583, Validation accuracy 98.02\n",
            "-----------------------------------------------------\n",
            "Epoch: 48\n",
            "\t Training loss 0.01148, Training accuracy 99.53\n",
            "\t Validation loss 0.00582, Validation accuracy 98.16\n",
            "-----------------------------------------------------\n",
            "Epoch: 49\n",
            "\t Training loss 0.01148, Training accuracy 99.57\n",
            "\t Validation loss 0.00582, Validation accuracy 98.12\n",
            "-----------------------------------------------------\n",
            "Epoch: 50\n",
            "\t Training loss 0.01148, Training accuracy 99.56\n",
            "\t Validation loss 0.00582, Validation accuracy 98.25\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.01148, Training accuracy 99.61\n",
            "\t Validation loss 0.00582, Validation accuracy 98.25\n",
            "\t Test loss 0.00591, Test accuracy 98.41\n",
            "-----------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
